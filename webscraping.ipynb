{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddde9f60",
   "metadata": {},
   "source": [
    "# rohe beschreibung was in den codes gemacht wird und wieso\n",
    "\n",
    "oft wird rag gebaut mit pdfs, und wenn man so ein system aufbaut kann man sicherlich eine grÃ¶ssere user base ansprechen, welche das system dann auch bauen mÃ¶chten. in diesem projekt mÃ¶chte ich aber webscraping von yahoo finance machen, weil viele leute auch yahoo finance lesen und auf yahoo finance viel mehr informationen sind. ich kann dort webscraping machen von bestimmten aktien, fonds oder indexes und dann alle informationen scrapen von kursen, artikeln, stock prices,\n",
    "intraday trading data,\n",
    "option prices;\n",
    "company disclosures and information such as :\n",
    "financial information,\n",
    "earnings,\n",
    "analyst ratings and recommendations,\n",
    "insider trades,\n",
    "SEC filings,\n",
    "news,\n",
    "large shareholders;\n",
    "\n",
    " dann kann ich all das in einen ordner speichern und einen rag bauen auf all diese sachen. das spart einerseits zeit weil man nicht manuell nach pdfs online suchen muss zu den gewÃ¼nschten themen, sondern man kann alles zeitsparend an einem ort scrapen und dann weiterbenutzen als input fÃ¼r das rag.\n",
    "\n",
    " denn technisch gesehen macht LangChain keinen Unterschied, ob deine Daten ursprÃ¼nglich aus PDFs, TXTs, Webseiten oder JSON stammen. Wichtig ist nur, was in den vektorstore eingespeist wird, ist reiner text, nicht das ursprÃ¼ngliche Dateiformat (wie z.b. pdf).\n",
    "\n",
    " Ob du folgenden Text aus einem PDF extrahierst:\n",
    " In 2023, Allianz reported a revenue of â‚¬152.7 billion and a net income of â‚¬14.2 billion.\n",
    "\n",
    "oder denselben Text aus einem JSON-Feld zusammenstellst wie:\n",
    "{ \"revenue\": \"â‚¬152.7 billion\", \"net_income\": \"â‚¬14.2 billion\" }\n",
    "\n",
    "ist fÃ¼r den Vektorstore und LangChain vÃ¶llig egal, solange du am Ende diesen Text als Chunk Ã¼bergibst.\n",
    "\n",
    "ðŸ“˜ Warum sieht man online meist PDFs?\n",
    "\n",
    "PDFs sind realistische Quellen (z.â€¯B. GeschÃ¤ftsberichte, Studien)\n",
    "Sie demonstrieren komplexere Verarbeitung (PDF â†’ Text â†’ Chunk â†’ Embedding)\n",
    "Viele Nutzer haben genau solche Dateien â†’ Tutorials wollen fÃ¼r die breite Masse relevant sein\n",
    "JSON ist oft maschinenlesbar, aber in der Praxis weniger \"typisch\" fÃ¼r Endnutzer\n",
    "âœ… Wann ist JSON sogar besser?\n",
    "\n",
    "Wenn du strukturierte, tabellarische Daten hast (Finanzdaten, ESG-KPIs, Zeitreihen)\n",
    "Wenn du Daten aus APIs sammelst (z.â€¯B. von Morningstar, AlphaVantage, Unternehmensdatenbanken)\n",
    "Wenn du dein System voll automatisiert aufbaust ohne manuelles PDF-Verarbeiten\n",
    "Wenn du bestimmte Felder gezielt extrahieren und in Textform bringen willst\n",
    "\n",
    "Ja, du kannst problemlos ein LangChain-basiertes RAG-System mit JSON-Dateien aufbauen.\n",
    "ðŸ§± Die einzige Voraussetzung: Du musst den Inhalt in Klartext-Chunks bringen, damit das LLM damit arbeiten kann.\n",
    "\n",
    "fÃ¼r jede firma und fÃ¼r jeden datensatz (aktienkurs, cashflow etc.) wird jeweils ein json datei generiert aus folgendem grund:\n",
    "\n",
    "1. Modular & skalierbar\n",
    "\n",
    "Wenn jede Bank ihr eigenes Verzeichnis bzw. JSON-Set hat, kannst du:\n",
    "\n",
    "gezielt einzelne Firmen updaten (nur JPM aktualisieren statt alles neu scrapen)\n",
    "schneller auf firmenspezifische Daten zugreifen\n",
    "einfacher firmenspezifische Claims prÃ¼fen (z.â€¯B. â€žWie war der Cashflow von JPM?â€œ)\n",
    "\n",
    "2. Sauberer fÃ¼r Chunking & RAG\n",
    "\n",
    "Ein RAG-System arbeitet oft dokumentenbasiert. Wenn jede Firma ein â€žDokumentâ€œ bzw. Datensatz ist, kannst du:\n",
    "\n",
    "firmenspezifische Chunks erzeugen\n",
    "Kontext klar steuern: \"Context: JPMorgan\" â†’ keine Gefahr durch â€žvermischteâ€œ Daten\n",
    "Retrieval filtern nach company_name, ticker, etc.\n",
    "\n",
    "3. Einfachere Annotation / Fact-Checking\n",
    "\n",
    "FÃ¼r deinen Fragebogen willst du Claims wie:\n",
    "\n",
    "â€žJPMorgan hatte 2024 einen positiven Free Cash Flow.â€œ\n",
    "â†’ Da ist es superpraktisch, wenn du direkt auf JPM/cashflow.json zugreifen kannst â€“ statt alles aus einem Riesendokument zu parsen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b328dd",
   "metadata": {},
   "source": [
    "# source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "108b3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import os\n",
    "import json\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95737992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Worked: JPM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in: data/jpm/jpm_shareprices.json\n",
      "Saved in: data/jpm/jpm_info.json\n",
      "Saved in: data/jpm/jpm_news.json\n",
      "Saved in: data/jpm/jpm_earning_dates.json\n",
      "Saved in: data/jpm/jpm_insider_transactions.json\n",
      "Saved in: data/jpm/jpm_institutional_holders.json\n",
      "Saved in: data/jpm/jpm_mutualfund_holders.json\n",
      "Saved in: data/jpm/jpm_cashflow.json\n",
      "Saved in: data/jpm/jpm_balance_sheet.json\n",
      "Saved in: data/jpm/jpm_financials.json\n",
      "\n",
      " Worked: BAC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in: data/bac/bac_shareprices.json\n",
      "Saved in: data/bac/bac_info.json\n",
      "Saved in: data/bac/bac_news.json\n",
      "Saved in: data/bac/bac_earning_dates.json\n",
      "Saved in: data/bac/bac_insider_transactions.json\n",
      "Saved in: data/bac/bac_institutional_holders.json\n",
      "Saved in: data/bac/bac_mutualfund_holders.json\n",
      "Saved in: data/bac/bac_cashflow.json\n",
      "Saved in: data/bac/bac_balance_sheet.json\n",
      "Saved in: data/bac/bac_financials.json\n",
      "\n",
      " Worked: C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in: data/citi/citi_shareprices.json\n",
      "Saved in: data/citi/citi_info.json\n",
      "Saved in: data/citi/citi_news.json\n",
      "Saved in: data/citi/citi_earning_dates.json\n",
      "Saved in: data/citi/citi_insider_transactions.json\n",
      "Saved in: data/citi/citi_institutional_holders.json\n",
      "Saved in: data/citi/citi_mutualfund_holders.json\n",
      "Saved in: data/citi/citi_cashflow.json\n",
      "Saved in: data/citi/citi_balance_sheet.json\n",
      "Saved in: data/citi/citi_financials.json\n",
      "\n",
      " Worked: UBSG.SW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in: data/ubs/ubsg_shareprices.json\n",
      "Saved in: data/ubs/ubsg_info.json\n",
      "Saved in: data/ubs/ubsg_news.json\n",
      "Saved in: data/ubs/ubsg_earning_dates.json\n",
      "Empty DataFrame: data/ubs/ubsg_insider_transactions.json\n",
      "Saved in: data/ubs/ubsg_institutional_holders.json\n",
      "Saved in: data/ubs/ubsg_mutualfund_holders.json\n",
      "Saved in: data/ubs/ubsg_cashflow.json\n",
      "Saved in: data/ubs/ubsg_balance_sheet.json\n",
      "Saved in: data/ubs/ubsg_financials.json\n",
      "\n",
      " Worked: HSBA.L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in: data/hsbc/hsbc_shareprices.json\n",
      "Saved in: data/hsbc/hsbc_info.json\n",
      "Saved in: data/hsbc/hsbc_news.json\n",
      "Saved in: data/hsbc/hsbc_earning_dates.json\n",
      "Saved in: data/hsbc/hsbc_insider_transactions.json\n",
      "Saved in: data/hsbc/hsbc_institutional_holders.json\n",
      "Saved in: data/hsbc/hsbc_mutualfund_holders.json\n",
      "Saved in: data/hsbc/hsbc_cashflow.json\n",
      "Saved in: data/hsbc/hsbc_balance_sheet.json\n",
      "Saved in: data/hsbc/hsbc_financials.json\n",
      "\n",
      " Worked: BNP.PA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in: data/bnp/bnp_shareprices.json\n",
      "Saved in: data/bnp/bnp_info.json\n",
      "Saved in: data/bnp/bnp_news.json\n",
      "Saved in: data/bnp/bnp_earning_dates.json\n",
      "Empty DataFrame: data/bnp/bnp_insider_transactions.json\n",
      "Saved in: data/bnp/bnp_institutional_holders.json\n",
      "Saved in: data/bnp/bnp_mutualfund_holders.json\n",
      "Saved in: data/bnp/bnp_cashflow.json\n",
      "Saved in: data/bnp/bnp_balance_sheet.json\n",
      "Saved in: data/bnp/bnp_financials.json\n"
     ]
    }
   ],
   "source": [
    "# masterfile, das alles webscraped mit yfinance und in einzelnen json files pro unternehmen abspeichert\n",
    "\n",
    "\n",
    "tickers = ['JPM', 'BAC', 'C', 'UBSG.SW', 'HSBA.L', 'BNP.PA']\n",
    "\n",
    "filename_prefixes = {\n",
    "    'JPM': 'jpm_',\n",
    "    'BAC': 'bac_',\n",
    "    'C': 'citi_',\n",
    "    'UBSG.SW': 'ubsg_',\n",
    "    'HSBA.L': 'hsbc_',\n",
    "    'BNP.PA': 'bnp_'\n",
    "}\n",
    "\n",
    "folder_names = {\n",
    "    'JPM': 'jpm',\n",
    "    'BAC': 'bac',\n",
    "    'C': 'citi',\n",
    "    'UBSG.SW': 'ubs',\n",
    "    'HSBA.L': 'hsbc',\n",
    "    'BNP.PA': 'bnp'\n",
    "}\n",
    "\n",
    "base_path = \"data\"\n",
    "\n",
    "# save json files with datetime\n",
    "def save_json(data, path):\n",
    "    def json_converter(o):\n",
    "        if isinstance(o, (datetime.datetime, datetime.date)):\n",
    "            return o.isoformat()\n",
    "        return str(o)\n",
    "\n",
    "    try:\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            if data.empty:\n",
    "                print(f\"Empty DataFrame: {path}\")\n",
    "                return\n",
    "            data = data.reset_index()\n",
    "            data.columns = [str(col) for col in data.columns]\n",
    "            for col in data.columns:\n",
    "                if pd.api.types.is_datetime64_any_dtype(data[col]):\n",
    "                    data[col] = pd.to_datetime(data[col]).dt.strftime('%Y-%m-%d')\n",
    "            data = data.to_dict(orient=\"records\")\n",
    "        elif isinstance(data, (dict, list)):\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"No serialization: {path}\")\n",
    "            return\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False, default=json_converter)\n",
    "\n",
    "        print(f\"Saved in: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while saving {path}: {e}\")\n",
    "\n",
    "# get and save data for each ticker\n",
    "for ticker in tickers:\n",
    "    print(f\"\\n Worked: {ticker}\")\n",
    "    prefix = filename_prefixes[ticker]\n",
    "    folder = folder_names[ticker]\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # stock prices via yf.download\n",
    "        hist = yf.download(ticker, group_by=\"ticker\", auto_adjust=False)\n",
    "        hist.reset_index(inplace=True)\n",
    "        hist['Date'] = pd.to_datetime(hist['Date']).dt.strftime('%Y-%m-%d')\n",
    "        save_json(hist, os.path.join(folder_path, f\"{prefix}shareprices.json\"))\n",
    "    except Exception as e:\n",
    "        print(f\"shareprices error {ticker}: {e}\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        t = yf.Ticker(ticker)\n",
    "\n",
    "        # Informations\n",
    "        save_json(t.info, os.path.join(folder_path, f\"{prefix}info.json\"))\n",
    "\n",
    "        # News\n",
    "        save_json(t.news, os.path.join(folder_path, f\"{prefix}news.json\"))\n",
    "\n",
    "        # Earnings\n",
    "        save_json(t.earnings_dates, os.path.join(folder_path, f\"{prefix}earning_dates.json\"))\n",
    "\n",
    "        # Insider Transactions\n",
    "        save_json(t.insider_transactions, os.path.join(folder_path, f\"{prefix}insider_transactions.json\"))\n",
    "\n",
    "        # Institutional Holders\n",
    "        save_json(t.institutional_holders, os.path.join(folder_path, f\"{prefix}institutional_holders.json\"))\n",
    "\n",
    "        # Mutual Fund Holders\n",
    "        save_json(t.mutualfund_holders, os.path.join(folder_path, f\"{prefix}mutualfund_holders.json\"))\n",
    "\n",
    "        # Cashflow\n",
    "        save_json(t.cashflow.transpose(), os.path.join(folder_path, f\"{prefix}cashflow.json\"))\n",
    "\n",
    "        # Balance Sheet\n",
    "        save_json(t.balance_sheet.transpose(), os.path.join(folder_path, f\"{prefix}balance_sheet.json\"))\n",
    "\n",
    "        # Financial Data\n",
    "        save_json(t.financials.transpose(), os.path.join(folder_path, f\"{prefix}financials.json\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ticker error {ticker}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILED TRY OUT: artikel scrapen online und alle titel und url abspeichern in einem einzigen json file pro unternehmen\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "def search_yahoo_news_by_keyword(keyword, max_articles=1000, step=20):\n",
    "    base_url = \"https://query1.finance.yahoo.com/v1/finance/search\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    all_articles = []\n",
    "    start = 0\n",
    "\n",
    "    while len(all_articles) < max_articles:\n",
    "        params = {\n",
    "            \"q\": keyword,\n",
    "            \"newsCount\": step,\n",
    "            \"start\": start\n",
    "        }\n",
    "\n",
    "        res = requests.get(base_url, headers=headers, params=params)\n",
    "        if res.status_code != 200:\n",
    "            print(f\"getting data error (Statuscode {res.status_code})\")\n",
    "            break\n",
    "\n",
    "        data = res.json()\n",
    "        news_batch = data.get(\"news\", [])\n",
    "        if not news_batch:\n",
    "            print(\"no more articles found.\")\n",
    "            break\n",
    "\n",
    "        for item in news_batch:\n",
    "            title = item.get(\"title\", \"\")\n",
    "            if keyword.lower() in title.lower():\n",
    "                all_articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"url\": item.get(\"link\"),\n",
    "                    \"publisher\": item.get(\"publisher\"),\n",
    "                    \"providerPublishTime\": item.get(\"providerPublishTime\")\n",
    "                })\n",
    "\n",
    "        print(f\"ðŸ”Ž {len(all_articles)} found articles...\")\n",
    "        start += step\n",
    "        time.sleep(1)\n",
    "\n",
    "    return all_articles[:max_articles]\n",
    "\n",
    "def save_news_as_json(news_list, keyword):\n",
    "    folder = \"yahoo_news_filtered\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    file_path = os.path.join(folder, f\"{keyword}_filtered_news.json\")\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(news_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… {len(news_list)} save articles in: {file_path}\")\n",
    "\n",
    "# search for this key word\n",
    "keyword = input(\"UBS\").strip()\n",
    "\n",
    "# get and save article\n",
    "filtered_news = search_yahoo_news_by_keyword(keyword, max_articles=1000)\n",
    "save_news_as_json(filtered_news, keyword)\n",
    "\n",
    "# ------------------------------\n",
    "# url nehmen und dann den text extrahieren und speichern\n",
    "from newspaper import Article\n",
    "\n",
    "def extract_article_content(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    return {\n",
    "        \"title\": article.title,\n",
    "        \"text\": article.text\n",
    "    }\n",
    "\n",
    "# Beispiel-URL (ersetzen durch deine eigene)\n",
    "url = input(\"enter article url: \").strip()\n",
    "\n",
    "try:\n",
    "    result = extract_article_content(url)\n",
    "    print(\"\\ntitle:\")\n",
    "    print(result[\"title\"])\n",
    "    print(\"\\narticleText:\")\n",
    "    print(result[\"text\"])\n",
    "except Exception as e:\n",
    "    print(\"error while extracting article:\", e)\n",
    "\n",
    "\n",
    "\n",
    "# diese idee wurde aber verworfen, denn als ich 1000 artikel scrapen wollte kamen nur ca 20 raus von einem einzigen tag und der rest waren einfach kopien.\n",
    "# scraping recht eingeschrÃ¤nkt. darum mache ich das nun manuell und somit kann ich alles holen was ich effektiv brauche. \n",
    "# der fokus der arbeit liegt ja bei fact checking llm darum hole ich lieber manuell gute daten und weiss dass diese stimmen und kann dann einen besseren benchmark haben \n",
    "# und bessere fragen stellen fÃ¼r fragebogen mit besseren antwortgenauigkeit das ist das wichtigste um die LLMs dann zu fact checken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87e55e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get articles for: Bank of America\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/lln_4j7d4976cj9dpf8gkcsh0000gn/T/ipykernel_2037/1691552168.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"last_updated\": datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: data/bac/bac_wikipedia.json\n",
      "Get articles for: BNP Paribas\n",
      "saved: data/bnp/bnp_wikipedia.json\n",
      "Get articles for: Citigroup\n",
      "saved: data/citi/citi_wikipedia.json\n",
      "Get articles for: HSBC\n",
      "saved: data/hsbc/hsbc_wikipedia.json\n",
      "Get articles for: JPMorgan Chase\n",
      "saved: data/jpm/jpm_wikipedia.json\n",
      "Get articles for: UBS\n",
      "saved: data/ubs/ubsg_wikipedia.json\n"
     ]
    }
   ],
   "source": [
    "# scrape wikipedia sites\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_wikipedia_article(title, lang=\"en\"):\n",
    "\n",
    "    url = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts|info\",\n",
    "        \"titles\": title,\n",
    "        \"explaintext\": True,\n",
    "        \"inprop\": \"url\"\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params=params)\n",
    "    data = res.json()\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page = next(iter(pages.values()))\n",
    "\n",
    "    article_info = {\n",
    "        \"title\": page.get(\"title\"),\n",
    "        \"url\": page.get(\"fullurl\"),\n",
    "        \"extract\": page.get(\"extract\"),\n",
    "        \"last_updated\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "\n",
    "    return article_info\n",
    "\n",
    "def save_article_to_json(article_info, folder_path, filename):\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(article_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"saved: {file_path}\")\n",
    "\n",
    "folder_names = {\n",
    "    'JPM': 'jpm',\n",
    "    'BAC': 'bac',\n",
    "    'C': 'citi',\n",
    "    'UBSG.SW': 'ubs',\n",
    "    'HSBA.L': 'hsbc',\n",
    "    'BNP.PA': 'bnp'\n",
    "}\n",
    "\n",
    "file_prefixes = {\n",
    "    'JPM': 'jpm_',\n",
    "    'BAC': 'bac_',\n",
    "    'C': 'citi_',\n",
    "    'UBSG.SW': 'ubsg_',\n",
    "    'HSBA.L': 'hsbc_',\n",
    "    'BNP.PA': 'bnp_'\n",
    "}\n",
    "\n",
    "banks = [\n",
    "    {\"wikipedia_title\": \"Bank of America\", \"ticker\": \"BAC\"},\n",
    "    {\"wikipedia_title\": \"BNP Paribas\", \"ticker\": \"BNP.PA\"},\n",
    "    {\"wikipedia_title\": \"Citigroup\", \"ticker\": \"C\"},\n",
    "    {\"wikipedia_title\": \"HSBC\", \"ticker\": \"HSBA.L\"},\n",
    "    {\"wikipedia_title\": \"JPMorgan Chase\", \"ticker\": \"JPM\"},\n",
    "    {\"wikipedia_title\": \"UBS\", \"ticker\": \"UBSG.SW\"}\n",
    "]\n",
    "\n",
    "for bank in banks:\n",
    "    ticker = bank[\"ticker\"]\n",
    "    print(f\"Get articles for: {bank['wikipedia_title']}\")\n",
    "\n",
    "    try:\n",
    "        article = get_wikipedia_article(bank[\"wikipedia_title\"], lang=\"en\")\n",
    "\n",
    "        folder_path = os.path.join(\"data\", folder_names[ticker])\n",
    "        filename = file_prefixes[ticker] + \"wikipedia.json\"\n",
    "\n",
    "        save_article_to_json(\n",
    "            article_info=article,\n",
    "            folder_path=folder_path,\n",
    "            filename=filename\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {bank['wikipedia_title']}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
