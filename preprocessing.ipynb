{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682a83b3",
   "metadata": {},
   "source": [
    "erklärung von warum zu text ändern, chunks, embedding und vektor datenbank\n",
    "\n",
    "wir müssen pre processing machen, da praktisch alle json files, ausser das von wikipedia, als tabellen mit zahlen gespeichert sind. Die Inhalte für den Input Teil des RAG Systems müssen in Textform sein.\n",
    "(z.B. json, pdf, csv) & am besten als Strings pro Chunk.\n",
    "chunk = \"Im Jahr 2022 hatte UBS ein Eigenkapital von 56,8 Mrd. USD und Schulden von 193 Mrd. USD.\"\n",
    "\n",
    "das ist eben sehr wichtig für den schritt embeddings. embeddings sind vektor-repräsentationen eines textes, damit kann man ähnliche texte finden, auch wenn die wörter verschieden sind. Und man braucht sowieso alles in Textform, weil LLMs (wie ChatGPT) mit Text arbeiten, nicht mit Tabellen oder JSON direkt. LLMs verstehen nur Wörter und Sätze, nicht direkt Strukturen wie:\n",
    "{\"Eigenkapital\": 5000000000}\n",
    "mittlerweile können sich llms ein einfacheres bild machen und sagen datei titel ist ubs und gespeichert ist eigenkapital also kann man darauf schliessen dass eigenkapitel der ubs gemeint ist. \n",
    "beim rag wäre das aber sehr schwer umzusetzen und darum muss man alles so gut wie möglich in text haben um das dann in chunks zu speichern.\n",
    "\n",
    "Wenn du fragst:\n",
    "\n",
    "Wie hoch war das Eigenkapital von UBS 2022?\n",
    "Der LLM braucht einen passenden Text-Abschnitt (Chunk), damit er die Antwort darin erkennt.\n",
    "→ Rohdaten wie \"Eigenkapital\": 5000000000 sagen ihm nichts – er braucht Kontext in Sprache.\n",
    "\n",
    "🔹 Chunk = ein sinnvoller Textabschnitt, typischerweise:\n",
    "1–3 Sätze\n",
    "oder ein Absatz\n",
    "oder eine Aussage mit Bedeutung\n",
    "\n",
    "❌ Kein Chunk:\n",
    "\"Eigenkapital\"\n",
    "\"5000000000\"\n",
    "→ Das ist für das Modell zu wenig Kontext. Es weiß nicht: „Was ist 5 Mrd.? Für wen? Wann?“\n",
    "\n",
    "✅ Besserer Chunk:\n",
    "\"Im Jahr 2022 betrug das Eigenkapital von UBS 5 Milliarden USD.\"\n",
    "→ Das ist ein vollständiger, verständlicher „Textblock“, der in Embeddings übersetzt werden kann.\n",
    "\n",
    "❓ 3. Was ist ein Embedding?\n",
    "\n",
    "🧠 Embedding = ein Zahlen-Vektor, der die Bedeutung eines Textes beschreibt\n",
    "Stell dir vor:\n",
    "\n",
    "Ein Text wie „UBS hat 5 Mrd. Eigenkapital“ → wird in einen Vektor mit 768 Zahlen umgewandelt.\n",
    "Das ist wie ein „Fingerabdruck der Bedeutung“ dieses Textes.\n",
    "📦 Beispiel:\n",
    "\"Das Eigenkapital beträgt 5 Mrd.\" → [0.12, -0.88, 0.03, ..., 0.41]\n",
    "\"UBS hat ein Eigenkapital von 5 Milliarden.\" → sehr ähnlicher Vektor!\n",
    "→ So kann ein System erkennen: „Ah, das bedeutet dasselbe!“\n",
    "\n",
    "❓ 4. Was ist eine Vektor-Datenbank?\n",
    "\n",
    "🧠 Eine Vektor-Datenbank speichert:\n",
    "viele solcher Embeddings (= Vektoren)\n",
    "verknüpft mit deinem Originaltext\n",
    "📍 Beispiel:\n",
    "\n",
    "Vektor (Embedding)\tText-Chunk\n",
    "[0.12, -0.88, 0.03, ...]\t\"UBS hatte 2022 ein Eigenkapital...\"\n",
    "[-0.23, 0.15, -0.44, ...]\t\"JPMorgan meldete 2019 einen Gewinn...\"\n",
    "Wenn du eine Frage stellst, z. B.:\n",
    "\n",
    "\"Wie hoch war das Eigenkapital von UBS 2022?\"\n",
    "Dann wird die Frage auch in einen Vektor umgewandelt – und die Datenbank findet den ähnlichsten Chunk, der dazu passt.\n",
    "\n",
    "✅ Kurz zusammengefasst\n",
    "\n",
    "\n",
    "Konzept\tErklärung\n",
    "Textform\tGPT versteht nur Sprache, keine Zahlenlisten oder Tabellen direkt\n",
    "Chunk\tEin sinnvoller Textabschnitt mit Kontext, z. B. 1–3 Sätze\n",
    "Embedding\tZahlencode, der die Bedeutung eines Textes beschreibt (z. B. 768 Zahlen)\n",
    "Vektor-Datenbank\tSucht nach dem „bedeutungsmäßig ähnlichsten“ Text\n",
    "\n",
    "\n",
    "beispiel von pre processing:\n",
    "\n",
    "Rohdaten:\n",
    "\n",
    "{\n",
    "  \"index\": \"2024-12-31\",\n",
    "  \"Total Assets\": 1565028000000.0,\n",
    "  \"Cash And Cash Equivalents\": 223329000000.0,\n",
    "  \"Total Debt\": 322128000000.0,\n",
    "  \"Common Stock Equity\": 85079000000.0\n",
    "}\n",
    "→ Wird zu:\n",
    "\n",
    "Am 31. Dezember 2024 hatte UBS insgesamt 1,56 Billionen USD an Vermögenswerten. Die liquiden Mittel betrugen 223,33 Milliarden USD. Die gesamten Schulden beliefen sich auf 322,13 Milliarden USD. Das Eigenkapital der Aktionäre lag bei 85,08 Milliarden USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cfc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 5 chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_balance_sheet_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für balance_sheet (pfad und bank muss jeweils angepasst werden)\n",
    "# musste es so lösen, weil wenn ich alles alle banken und alle json files gibt es probleme, \n",
    "# weil Diese enthalten Fließtext oder komplex strukturierte Inhalte, die wir anders verarbeiten müssen als die tabellarischen Zeitreihen. \n",
    "# da alle typen anderst gemacht werden müssen werde ich ein skript pro file schreiben\n",
    "\n",
    "# Das programmiert so wie hier jetzt unten ist der perfekte Ansatz, wenn du ganz sicherstellen willst, dass jede Dateiart korrekt verarbeitet wird – sauber, strukturiert, reproduzierbar 💯\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# SET YOUR FILE PATH HERE (manually one at a time)\n",
    "FILE_PATH = \"data/ubs/ubsg_balance_sheet.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"balance_sheet\"\n",
    "\n",
    "# Define output path\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Fields to include in chunks\n",
    "RELEVANT_FIELDS = {\n",
    "    \"Total Assets\": \"total assets\",\n",
    "    \"Cash And Cash Equivalents\": \"cash and cash equivalents\",\n",
    "    \"Total Debt\": \"total debt\",\n",
    "    \"Common Stock Equity\": \"common stock equity\",\n",
    "    \"Tangible Book Value\": \"tangible book value\",\n",
    "    \"Net Debt\": \"net debt\"\n",
    "}\n",
    "\n",
    "# Format helper\n",
    "def format_number(value):\n",
    "    if value is None or value != value or (isinstance(value, float) and np.isnan(value)):\n",
    "        return \"not available\"\n",
    "    if abs(value) >= 1_000_000_000_000:\n",
    "        return f\"{value / 1_000_000_000_000:.2f} trillion USD\"\n",
    "    elif abs(value) >= 1_000_000_000:\n",
    "        return f\"{value / 1_000_000_000:.2f} billion USD\"\n",
    "    elif abs(value) >= 1_000_000:\n",
    "        return f\"{value / 1_000_000:.2f} million USD\"\n",
    "    else:\n",
    "        return f\"{value:.0f} USD\"\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for row in data:\n",
    "    date = row.get(\"index\") or \"unknown\"\n",
    "    lines = [f\"As of {date}, {BANK} reported:\"]\n",
    "    for key, label in RELEVANT_FIELDS.items():\n",
    "        value = format_number(row.get(key))\n",
    "        lines.append(f\"- {label.capitalize()}: {value}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save to /data_chunks\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(chunks)} chunks saved to:\\n{OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d4fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 4 chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_cashflow_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für cashflow (pfad und bank muss jeweils angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# SET YOUR FILE PATH HERE (manually one at a time)\n",
    "FILE_PATH = \"data/ubs/ubsg_cashflow.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"cashflow\"\n",
    "\n",
    "# Define output path\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Cashflow-relevant fields\n",
    "RELEVANT_FIELDS = {\n",
    "    \"Operating Cash Flow\": \"operating cash flow\",\n",
    "    \"Investing Cash Flow\": \"investing cash flow\",\n",
    "    \"Financing Cash Flow\": \"financing cash flow\",\n",
    "    \"Free Cash Flow\": \"free cash flow\",\n",
    "    \"Beginning Cash Position\": \"beginning cash position\",\n",
    "    \"End Cash Position\": \"end cash position\",\n",
    "    \"Changes In Cash\": \"change in total cash\"\n",
    "}\n",
    "\n",
    "# Format helper\n",
    "def format_number(value):\n",
    "    if value is None or value != value or (isinstance(value, float) and np.isnan(value)):\n",
    "        return \"not available\"\n",
    "    if abs(value) >= 1_000_000_000_000:\n",
    "        return f\"{value / 1_000_000_000_000:.2f} trillion USD\"\n",
    "    elif abs(value) >= 1_000_000_000:\n",
    "        return f\"{value / 1_000_000_000:.2f} billion USD\"\n",
    "    elif abs(value) >= 1_000_000:\n",
    "        return f\"{value / 1_000_000:.2f} million USD\"\n",
    "    else:\n",
    "        return f\"{value:.0f} USD\"\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for row in data:\n",
    "    date = row.get(\"index\") or \"unknown\"\n",
    "    lines = [f\"As of {date}, {BANK} cash flow report:\"]\n",
    "    for key, label in RELEVANT_FIELDS.items():\n",
    "        value = format_number(row.get(key))\n",
    "        lines.append(f\"- {label.capitalize()}: {value}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save to /data_chunks\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(chunks)} chunks saved to:\\n{OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd6157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 12 chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_earning_dates_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für earning_dates (pfad und bank muss jeweils angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Set your input path + bank name\n",
    "FILE_PATH = \"data/ubs/ubsg_earning_dates.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"earning_dates\"\n",
    "\n",
    "# Define output folder + file name\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Format helper\n",
    "def format_number(value, suffix=\"USD\"):\n",
    "    if value is None or value != value or (isinstance(value, float) and np.isnan(value)):\n",
    "        return \"not available\"\n",
    "    return f\"{value:.2f} {suffix}\"\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for row in data:\n",
    "    date = row.get(\"Earnings Date\", \"unknown\")\n",
    "    estimate = format_number(row.get(\"EPS Estimate\"), \"USD\")\n",
    "    reported = format_number(row.get(\"Reported EPS\"), \"USD\")\n",
    "    surprise = format_number(row.get(\"Surprise(%)\"), \"%\")\n",
    "\n",
    "    text = (\n",
    "        f\"On {date}, {BANK} reported earnings per share (EPS) of {reported}, \"\n",
    "        f\"compared to an estimate of {estimate}. \"\n",
    "        f\"The surprise was {surprise}.\"\n",
    "    )\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save to /data_chunks\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(chunks)} chunks saved to:\\n{OUTPUT_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632a4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 5 financial chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_financials_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für financials (pfad und bank muss jeweils angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Set your file path + bank info\n",
    "FILE_PATH = \"data/ubs/ubsg_financials.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"financials\"\n",
    "\n",
    "# Define output location\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Format helper\n",
    "def format_number(value):\n",
    "    if value is None or value != value or (isinstance(value, float) and np.isnan(value)):\n",
    "        return \"not available\"\n",
    "    if abs(value) >= 1_000_000_000_000:\n",
    "        return f\"{value / 1_000_000_000_000:.2f} trillion USD\"\n",
    "    elif abs(value) >= 1_000_000_000:\n",
    "        return f\"{value / 1_000_000_000:.2f} billion USD\"\n",
    "    elif abs(value) >= 1_000_000:\n",
    "        return f\"{value / 1_000_000:.2f} million USD\"\n",
    "    else:\n",
    "        return f\"{value:.0f} USD\"\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Generate chunks – all fields included\n",
    "chunks = []\n",
    "for row in data:\n",
    "    date = row.get(\"index\") or \"unknown\"\n",
    "    lines = [f\"As of {date}, {BANK} reported the following financials:\"]\n",
    "\n",
    "    for key, value in row.items():\n",
    "        if key == \"index\":\n",
    "            continue\n",
    "        value_str = format_number(value)\n",
    "        label = key.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "        lines.append(f\"- {label}: {value_str}\")\n",
    "\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save as JSONL\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(chunks)} financial chunks saved to:\\n{OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc5cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Info chunk saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_info_chunks.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/lln_4j7d4976cj9dpf8gkcsh0000gn/T/ipykernel_3234/603042743.py:49: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(int(value)).strftime('%Y-%m-%d')\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für info (pfad und bank muss jeweils angepasst manuell werden)\n",
    "\n",
    "# Du bekommst also standardmäßig YYYY-MM-DD, was optimal ist für:\n",
    "\n",
    "# maschinelles Parsen\n",
    "# Metadatenfilter\n",
    "# internationale LLMs & Vektor-DBs\n",
    "\n",
    "# ----------\n",
    "# Ein Unix-Timestamp wie 851990400 bedeutet:\n",
    "\n",
    "# Anzahl Sekunden seit dem 1. Januar 1970 um 00:00 UTC (Unix-Epoch)\n",
    "# Das ist ein weltweit genormter Zeitstandard – unabhängig von Land oder Format.\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Set file + bank info\n",
    "FILE_PATH = \"data/ubs/ubsg_info.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"info\"\n",
    "\n",
    "# Output path\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Format helpers\n",
    "def format_number(value):\n",
    "    if value is None or value != value or isinstance(value, (float, int)) and np.isnan(value):\n",
    "        return \"not available\"\n",
    "    if isinstance(value, (float, int)):\n",
    "        if abs(value) >= 1_000_000_000_000:\n",
    "            return f\"{value / 1_000_000_000_000:.2f} trillion USD\"\n",
    "        elif abs(value) >= 1_000_000_000:\n",
    "            return f\"{value / 1_000_000_000:.2f} billion USD\"\n",
    "        elif abs(value) >= 1_000_000:\n",
    "            return f\"{value / 1_000_000:.2f} million USD\"\n",
    "        else:\n",
    "            return f\"{value:.0f} USD\"\n",
    "    return str(value)\n",
    "\n",
    "def format_date(value):\n",
    "    try:\n",
    "        return datetime.utcfromtimestamp(int(value)).strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        return str(value)\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build fields from flat content\n",
    "flat_fields = []\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, (dict, list)):\n",
    "        continue\n",
    "\n",
    "    # Check for date fields\n",
    "    if \"date\" in key.lower() and isinstance(value, (int, float)):\n",
    "        value_str = format_date(value)\n",
    "    else:\n",
    "        value_str = format_number(value)\n",
    "\n",
    "    label = key.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "    flat_fields.append(f\"{label}: {value_str}\")\n",
    "\n",
    "# Add long summary at the beginning\n",
    "long_summary = data.get(\"longBusinessSummary\", None)\n",
    "if long_summary:\n",
    "    flat_fields.insert(0, long_summary)\n",
    "    flat_fields.insert(1, \"\\n--\\n\")\n",
    "\n",
    "# Add officer data\n",
    "officer_lines = []\n",
    "if \"companyOfficers\" in data and isinstance(data[\"companyOfficers\"], list):\n",
    "    for officer in data[\"companyOfficers\"]:\n",
    "        if \"name\" in officer:\n",
    "            name = officer[\"name\"]\n",
    "            title = officer.get(\"title\", \"unknown title\")\n",
    "            pay = format_number(officer.get(\"totalPay\", \"N/A\"))\n",
    "            officer_lines.append(f\"{name}, {title} – Total Pay: {pay}\")\n",
    "\n",
    "if officer_lines:\n",
    "    flat_fields.append(\"\\nTop Executives:\")\n",
    "    flat_fields.extend(officer_lines)\n",
    "\n",
    "# Final chunk text\n",
    "text = \"\\n\".join(flat_fields)\n",
    "\n",
    "chunk = {\n",
    "    \"text\": text,\n",
    "    \"metadata\": {\n",
    "        \"bank\": BANK,\n",
    "        \"date\": \"n/a\",\n",
    "        \"source\": SOURCE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(chunk, out_file, ensure_ascii=False)\n",
    "    out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Info chunk saved to:\\n{OUTPUT_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a4acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 150 insider transaction chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/jpm_insider_transactions_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für insider_transactions (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Datei-Pfade anpassen\n",
    "FILE_PATH = \"data/jpm/jpm_insider_transactions.json\"\n",
    "BANK = \"JPM\"\n",
    "SOURCE = \"insider_transactions\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# Lade JSON als DataFrame\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# NaNs ersetzen durch \"not available\"\n",
    "df = df.fillna(\"not available\")\n",
    "\n",
    "# Generiere Chunks\n",
    "chunks = []\n",
    "for _, row in df.iterrows():\n",
    "    date_str = row[\"Start Date\"]\n",
    "    text_lines = []\n",
    "    for col, val in row.items():\n",
    "        if col != \"index\" and col != \"Start Date\":\n",
    "            label = col.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "            text_lines.append(f\"{label}: {val}\")\n",
    "    chunk_text = \"\\n\".join(text_lines)\n",
    "\n",
    "    chunk = {\n",
    "        \"text\": chunk_text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date_str,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    }\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Speichern\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(chunks)} insider transaction chunks saved to:\\n{OUTPUT_FILE}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22044f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 10 institutional holder chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_institutional_holders_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für institutional_holders (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# File setup\n",
    "FILE_PATH = \"data/ubs/ubsg_institutional_holders.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"institutional_holders\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# Load as DataFrame\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "df = pd.DataFrame(records)\n",
    "df = df.fillna(\"not available\")\n",
    "\n",
    "# Build chunks\n",
    "chunks = []\n",
    "for _, row in df.iterrows():\n",
    "    date_str = row[\"Date Reported\"]\n",
    "    lines = []\n",
    "    for col, val in row.items():\n",
    "        if col in [\"index\", \"Date Reported\"]:\n",
    "            continue\n",
    "        label = col.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "        lines.append(f\"{label}: {val}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunk = {\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date_str,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    }\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Save as JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(chunks)} institutional holder chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52474fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 10 mutual fund holder chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_mutualfund_holders_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für mutualfund_holders (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# File paths & info\n",
    "FILE_PATH = \"data/ubs/ubsg_mutualfund_holders.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"mutualfund_holders\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# Load data\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "df = pd.DataFrame(records)\n",
    "df = df.fillna(\"not available\")\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for _, row in df.iterrows():\n",
    "    date_str = row[\"Date Reported\"]\n",
    "    lines = []\n",
    "    for col, val in row.items():\n",
    "        if col in [\"index\", \"Date Reported\"]:\n",
    "            continue\n",
    "        label = col.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "        lines.append(f\"{label}: {val}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date_str,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save as .jsonl\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(chunks)} mutual fund holder chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b11e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 10 news chunks saved to:\n",
      "data_chunks/jpm_news_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für news (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Input & Output Setup\n",
    "FILE_PATH = \"data/jpm/jpm_news.json\"\n",
    "BANK = \"JPM\"\n",
    "SOURCE = \"news\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for item in records:\n",
    "    content = item.get(\"content\", {})\n",
    "    pub_date = content.get(\"pubDate\", \"unknown\")\n",
    "    title = content.get(\"title\", \"\").strip()\n",
    "    summary = content.get(\"summary\", \"\").strip()\n",
    "    thumbnail_url = content.get(\"thumbnail\", {}).get(\"originalUrl\", \"\")\n",
    "\n",
    "    # Textchunk zusammenbauen\n",
    "    text_parts = [\n",
    "        f\"Title: {title}\" if title else \"\",\n",
    "        f\"Summary: {summary}\" if summary else \"\",\n",
    "        f\"URL: {thumbnail_url}\" if thumbnail_url else \"\"\n",
    "    ]\n",
    "    chunk_text = \"\\n\".join([part for part in text_parts if part])\n",
    "\n",
    "    if chunk_text:  # Nur speichern wenn sinnvoller Inhalt vorhanden\n",
    "        chunk = {\n",
    "            \"text\": chunk_text,\n",
    "            \"metadata\": {\n",
    "                \"bank\": BANK,\n",
    "                \"date\": pub_date[:10],  # Nur YYYY-MM-DD\n",
    "                \"source\": SOURCE\n",
    "            }\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "\n",
    "# Save to .jsonl\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(chunks)} news chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79124da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 7576 share price chunks saved to:\n",
      "data_chunks/ubsg_shareprices_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für shareprices (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 🔧 Pfade anpassen\n",
    "FILE_PATH = \"data/ubs/ubsg_shareprices.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"shareprices\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# JSON laden\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for row in data:\n",
    "    # Keys reparieren, falls Tuple-Strings wie \"('BAC', 'Open')\"\n",
    "    fixed_row = {}\n",
    "    for k, v in row.items():\n",
    "        if isinstance(k, str):\n",
    "            key_str = k\n",
    "        elif isinstance(k, (list, tuple)):\n",
    "            key_str = \"_\".join(str(part) for part in k)\n",
    "        else:\n",
    "            key_str = str(k)\n",
    "        fixed_row[key_str] = v\n",
    "\n",
    "    # Datum extrahieren – versuche alle Varianten\n",
    "    possible_date_keys = [k for k in fixed_row.keys() if \"date\" in k.lower()]\n",
    "    date = \"unknown\"\n",
    "    for dk in possible_date_keys:\n",
    "        date_candidate = fixed_row.get(dk)\n",
    "        if isinstance(date_candidate, str) and len(date_candidate) >= 10:\n",
    "            date = date_candidate[:10]\n",
    "            break\n",
    "\n",
    "    # Text zusammenbauen\n",
    "    lines = [f\"Date: {date}\"]\n",
    "    for key, value in fixed_row.items():\n",
    "        if key.lower() not in [dk.lower() for dk in possible_date_keys]:\n",
    "            lines.append(f\"{key}: {value}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Speichern\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(chunks)} share price chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240a0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 53 Wikipedia chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_wikipedia_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator für wikipedia (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "# warum man split_text() braucht:\n",
    "# In vielen RAG-Systemen ist es üblich, lange Texte in kleinere Einheiten (\"Chunks\") zu teilen, z. B.:\n",
    "\n",
    "# bessere Vektoreinbettung (Embeddings sind oft auf ~500–1000 Token beschränkt)\n",
    "# präzisere Antworten auf Teilfragen\n",
    "# niedrigere Kontextlängenbelastung bei LLM-Anfragen\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# 🔧 Wikipedia JSON-File & Infos\n",
    "FILE_PATH = \"data/ubs/ubsg_wikipedia.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"wikipedia\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# JSON laden\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Datum formatieren\n",
    "def parse_iso_date(iso_str):\n",
    "    try:\n",
    "        return datetime.fromisoformat(iso_str).strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "date = parse_iso_date(data.get(\"last_updated\", \"\"))\n",
    "title = data.get(\"title\", \"Unknown\")\n",
    "url = data.get(\"url\", \"Unknown\")\n",
    "extract = data.get(\"extract\", \"\").strip()\n",
    "\n",
    "# Wikipedia-Text chunken (z. B. alle 1000 Wörter als eigener Chunk)\n",
    "def split_text(text, max_words=200):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk_text = \" \".join(words[i:i + max_words])\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "text_chunks = split_text(extract)\n",
    "\n",
    "# RAG-kompatible JSONL-Chunks\n",
    "output_chunks = []\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    output_chunks.append({\n",
    "        \"text\": f\"{title} (Wikipedia excerpt {i+1})\\n\\n{chunk}\",\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE,\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"chunk_id\": i + 1\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Speichern\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in output_chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ {len(output_chunks)} Wikipedia chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
