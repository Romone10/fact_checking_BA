{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682a83b3",
   "metadata": {},
   "source": [
    "erklÃ¤rung von warum zu text Ã¤ndern, chunks, embedding und vektor datenbank\n",
    "\n",
    "wir mÃ¼ssen pre processing machen, da praktisch alle json files, ausser das von wikipedia, als tabellen mit zahlen gespeichert sind. Die Inhalte fÃ¼r den Input Teil des RAG Systems mÃ¼ssen in Textform sein.\n",
    "(z.B. json, pdf, csv) & am besten als Strings pro Chunk.\n",
    "chunk = \"Im Jahr 2022 hatte UBS ein Eigenkapital von 56,8 Mrd. USD und Schulden von 193 Mrd. USD.\"\n",
    "\n",
    "das ist eben sehr wichtig fÃ¼r den schritt embeddings. embeddings sind vektor-reprÃ¤sentationen eines textes, damit kann man Ã¤hnliche texte finden, auch wenn die wÃ¶rter verschieden sind. Und man braucht sowieso alles in Textform, weil LLMs (wie ChatGPT) mit Text arbeiten, nicht mit Tabellen oder JSON direkt. LLMs verstehen nur WÃ¶rter und SÃ¤tze, nicht direkt Strukturen wie:\n",
    "{\"Eigenkapital\": 5000000000}\n",
    "mittlerweile kÃ¶nnen sich llms ein einfacheres bild machen und sagen datei titel ist ubs und gespeichert ist eigenkapital also kann man darauf schliessen dass eigenkapitel der ubs gemeint ist. \n",
    "beim rag wÃ¤re das aber sehr schwer umzusetzen und darum muss man alles so gut wie mÃ¶glich in text haben um das dann in chunks zu speichern.\n",
    "\n",
    "Wenn du fragst:\n",
    "\n",
    "Wie hoch war das Eigenkapital von UBS 2022?\n",
    "Der LLM braucht einen passenden Text-Abschnitt (Chunk), damit er die Antwort darin erkennt.\n",
    "â†’ Rohdaten wie \"Eigenkapital\": 5000000000 sagen ihm nichts â€“ er braucht Kontext in Sprache.\n",
    "\n",
    "ðŸ”¹ Chunk = ein sinnvoller Textabschnitt, typischerweise:\n",
    "1â€“3 SÃ¤tze\n",
    "oder ein Absatz\n",
    "oder eine Aussage mit Bedeutung\n",
    "\n",
    "âŒ Kein Chunk:\n",
    "\"Eigenkapital\"\n",
    "\"5000000000\"\n",
    "â†’ Das ist fÃ¼r das Modell zu wenig Kontext. Es weiÃŸ nicht: â€žWas ist 5 Mrd.? FÃ¼r wen? Wann?â€œ\n",
    "\n",
    "âœ… Besserer Chunk:\n",
    "\"Im Jahr 2022 betrug das Eigenkapital von UBS 5 Milliarden USD.\"\n",
    "â†’ Das ist ein vollstÃ¤ndiger, verstÃ¤ndlicher â€žTextblockâ€œ, der in Embeddings Ã¼bersetzt werden kann.\n",
    "\n",
    "â“ 3. Was ist ein Embedding?\n",
    "\n",
    "ðŸ§  Embedding = ein Zahlen-Vektor, der die Bedeutung eines Textes beschreibt\n",
    "Stell dir vor:\n",
    "\n",
    "Ein Text wie â€žUBS hat 5 Mrd. Eigenkapitalâ€œ â†’ wird in einen Vektor mit 768 Zahlen umgewandelt.\n",
    "Das ist wie ein â€žFingerabdruck der Bedeutungâ€œ dieses Textes.\n",
    "ðŸ“¦ Beispiel:\n",
    "\"Das Eigenkapital betrÃ¤gt 5 Mrd.\" â†’ [0.12, -0.88, 0.03, ..., 0.41]\n",
    "\"UBS hat ein Eigenkapital von 5 Milliarden.\" â†’ sehr Ã¤hnlicher Vektor!\n",
    "â†’ So kann ein System erkennen: â€žAh, das bedeutet dasselbe!â€œ\n",
    "\n",
    "â“ 4. Was ist eine Vektor-Datenbank?\n",
    "\n",
    "ðŸ§  Eine Vektor-Datenbank speichert:\n",
    "viele solcher Embeddings (= Vektoren)\n",
    "verknÃ¼pft mit deinem Originaltext\n",
    "ðŸ“ Beispiel:\n",
    "\n",
    "Vektor (Embedding)\tText-Chunk\n",
    "[0.12, -0.88, 0.03, ...]\t\"UBS hatte 2022 ein Eigenkapital...\"\n",
    "[-0.23, 0.15, -0.44, ...]\t\"JPMorgan meldete 2019 einen Gewinn...\"\n",
    "Wenn du eine Frage stellst, z.â€¯B.:\n",
    "\n",
    "\"Wie hoch war das Eigenkapital von UBS 2022?\"\n",
    "Dann wird die Frage auch in einen Vektor umgewandelt â€“ und die Datenbank findet den Ã¤hnlichsten Chunk, der dazu passt.\n",
    "\n",
    "âœ… Kurz zusammengefasst\n",
    "\n",
    "\n",
    "Konzept\tErklÃ¤rung\n",
    "Textform\tGPT versteht nur Sprache, keine Zahlenlisten oder Tabellen direkt\n",
    "Chunk\tEin sinnvoller Textabschnitt mit Kontext, z.â€¯B. 1â€“3 SÃ¤tze\n",
    "Embedding\tZahlencode, der die Bedeutung eines Textes beschreibt (z.â€¯B. 768 Zahlen)\n",
    "Vektor-Datenbank\tSucht nach dem â€žbedeutungsmÃ¤ÃŸig Ã¤hnlichstenâ€œ Text\n",
    "\n",
    "\n",
    "beispiel von pre processing:\n",
    "\n",
    "Rohdaten:\n",
    "\n",
    "{\n",
    "  \"index\": \"2024-12-31\",\n",
    "  \"Total Assets\": 1565028000000.0,\n",
    "  \"Cash And Cash Equivalents\": 223329000000.0,\n",
    "  \"Total Debt\": 322128000000.0,\n",
    "  \"Common Stock Equity\": 85079000000.0\n",
    "}\n",
    "â†’ Wird zu:\n",
    "\n",
    "Am 31. Dezember 2024 hatte UBS insgesamt 1,56 Billionen USD an VermÃ¶genswerten. Die liquiden Mittel betrugen 223,33 Milliarden USD. Die gesamten Schulden beliefen sich auf 322,13 Milliarden USD. Das Eigenkapital der AktionÃ¤re lag bei 85,08 Milliarden USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cfc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 5 chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_balance_sheet_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r balance_sheet (pfad und bank muss jeweils angepasst werden)\n",
    "# musste es so lÃ¶sen, weil wenn ich alles alle banken und alle json files gibt es probleme, \n",
    "# weil Diese enthalten FlieÃŸtext oder komplex strukturierte Inhalte, die wir anders verarbeiten mÃ¼ssen als die tabellarischen Zeitreihen. \n",
    "# da alle typen anderst gemacht werden mÃ¼ssen werde ich ein skript pro file schreiben\n",
    "\n",
    "# Das programmiert so wie hier jetzt unten ist der perfekte Ansatz, wenn du ganz sicherstellen willst, dass jede Dateiart korrekt verarbeitet wird â€“ sauber, strukturiert, reproduzierbar ðŸ’¯\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# SET YOUR FILE PATH HERE (manually one at a time)\n",
    "FILE_PATH = \"data/ubs/ubsg_balance_sheet.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"balance_sheet\"\n",
    "\n",
    "# Define output path\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Fields to include in chunks\n",
    "RELEVANT_FIELDS = {\n",
    "    \"Total Assets\": \"total assets\",\n",
    "    \"Cash And Cash Equivalents\": \"cash and cash equivalents\",\n",
    "    \"Total Debt\": \"total debt\",\n",
    "    \"Common Stock Equity\": \"common stock equity\",\n",
    "    \"Tangible Book Value\": \"tangible book value\",\n",
    "    \"Net Debt\": \"net debt\"\n",
    "}\n",
    "\n",
    "# Format helper\n",
    "def format_number(value):\n",
    "    if value is None or value != value or (isinstance(value, float) and np.isnan(value)):\n",
    "        return \"not available\"\n",
    "    if abs(value) >= 1_000_000_000_000:\n",
    "        return f\"{value / 1_000_000_000_000:.2f} trillion USD\"\n",
    "    elif abs(value) >= 1_000_000_000:\n",
    "        return f\"{value / 1_000_000_000:.2f} billion USD\"\n",
    "    elif abs(value) >= 1_000_000:\n",
    "        return f\"{value / 1_000_000:.2f} million USD\"\n",
    "    else:\n",
    "        return f\"{value:.0f} USD\"\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for row in data:\n",
    "    date = row.get(\"index\") or \"unknown\"\n",
    "    lines = [f\"As of {date}, {BANK} reported:\"]\n",
    "    for key, label in RELEVANT_FIELDS.items():\n",
    "        value = format_number(row.get(key))\n",
    "        lines.append(f\"- {label.capitalize()}: {value}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save to /data_chunks\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(chunks)} chunks saved to:\\n{OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d4fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 4 chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_cashflow_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r cashflow (pfad und bank muss jeweils angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# SET YOUR FILE PATH HERE (manually one at a time)\n",
    "FILE_PATH = \"data/ubs/ubsg_cashflow.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"cashflow\"\n",
    "\n",
    "# Define output path\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Cashflow-relevant fields\n",
    "RELEVANT_FIELDS = {\n",
    "    \"Operating Cash Flow\": \"operating cash flow\",\n",
    "    \"Investing Cash Flow\": \"investing cash flow\",\n",
    "    \"Financing Cash Flow\": \"financing cash flow\",\n",
    "    \"Free Cash Flow\": \"free cash flow\",\n",
    "    \"Beginning Cash Position\": \"beginning cash position\",\n",
    "    \"End Cash Position\": \"end cash position\",\n",
    "    \"Changes In Cash\": \"change in total cash\"\n",
    "}\n",
    "\n",
    "# Format helper\n",
    "def format_number(value):\n",
    "    if value is None or value != value or (isinstance(value, float) and np.isnan(value)):\n",
    "        return \"not available\"\n",
    "    if abs(value) >= 1_000_000_000_000:\n",
    "        return f\"{value / 1_000_000_000_000:.2f} trillion USD\"\n",
    "    elif abs(value) >= 1_000_000_000:\n",
    "        return f\"{value / 1_000_000_000:.2f} billion USD\"\n",
    "    elif abs(value) >= 1_000_000:\n",
    "        return f\"{value / 1_000_000:.2f} million USD\"\n",
    "    else:\n",
    "        return f\"{value:.0f} USD\"\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for row in data:\n",
    "    date = row.get(\"index\") or \"unknown\"\n",
    "    lines = [f\"As of {date}, {BANK} cash flow report:\"]\n",
    "    for key, label in RELEVANT_FIELDS.items():\n",
    "        value = format_number(row.get(key))\n",
    "        lines.append(f\"- {label.capitalize()}: {value}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save to /data_chunks\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(chunks)} chunks saved to:\\n{OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd6157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 12 chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_earning_dates_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r earning_dates (pfad und bank muss jeweils angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Set your input path + bank name\n",
    "FILE_PATH = \"data/ubs/ubsg_earning_dates.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"earning_dates\"\n",
    "\n",
    "# Define output folder + file name\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Format helper\n",
    "def format_number(value, suffix=\"USD\"):\n",
    "    if value is None or value != value or (isinstance(value, float) and np.isnan(value)):\n",
    "        return \"not available\"\n",
    "    return f\"{value:.2f} {suffix}\"\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for row in data:\n",
    "    date = row.get(\"Earnings Date\", \"unknown\")\n",
    "    estimate = format_number(row.get(\"EPS Estimate\"), \"USD\")\n",
    "    reported = format_number(row.get(\"Reported EPS\"), \"USD\")\n",
    "    surprise = format_number(row.get(\"Surprise(%)\"), \"%\")\n",
    "\n",
    "    text = (\n",
    "        f\"On {date}, {BANK} reported earnings per share (EPS) of {reported}, \"\n",
    "        f\"compared to an estimate of {estimate}. \"\n",
    "        f\"The surprise was {surprise}.\"\n",
    "    )\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save to /data_chunks\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(chunks)} chunks saved to:\\n{OUTPUT_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632a4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 5 financial chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_financials_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r financials (pfad und bank muss jeweils angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Set your file path + bank info\n",
    "FILE_PATH = \"data/ubs/ubsg_financials.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"financials\"\n",
    "\n",
    "# Define output location\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Format helper\n",
    "def format_number(value):\n",
    "    if value is None or value != value or (isinstance(value, float) and np.isnan(value)):\n",
    "        return \"not available\"\n",
    "    if abs(value) >= 1_000_000_000_000:\n",
    "        return f\"{value / 1_000_000_000_000:.2f} trillion USD\"\n",
    "    elif abs(value) >= 1_000_000_000:\n",
    "        return f\"{value / 1_000_000_000:.2f} billion USD\"\n",
    "    elif abs(value) >= 1_000_000:\n",
    "        return f\"{value / 1_000_000:.2f} million USD\"\n",
    "    else:\n",
    "        return f\"{value:.0f} USD\"\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Generate chunks â€“ all fields included\n",
    "chunks = []\n",
    "for row in data:\n",
    "    date = row.get(\"index\") or \"unknown\"\n",
    "    lines = [f\"As of {date}, {BANK} reported the following financials:\"]\n",
    "\n",
    "    for key, value in row.items():\n",
    "        if key == \"index\":\n",
    "            continue\n",
    "        value_str = format_number(value)\n",
    "        label = key.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "        lines.append(f\"- {label}: {value_str}\")\n",
    "\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save as JSONL\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(chunks)} financial chunks saved to:\\n{OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc5cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Info chunk saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_info_chunks.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/lln_4j7d4976cj9dpf8gkcsh0000gn/T/ipykernel_3234/603042743.py:49: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(int(value)).strftime('%Y-%m-%d')\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r info (pfad und bank muss jeweils angepasst manuell werden)\n",
    "\n",
    "# Du bekommst also standardmÃ¤ÃŸig YYYY-MM-DD, was optimal ist fÃ¼r:\n",
    "\n",
    "# maschinelles Parsen\n",
    "# Metadatenfilter\n",
    "# internationale LLMs & Vektor-DBs\n",
    "\n",
    "# ----------\n",
    "# Ein Unix-Timestamp wie 851990400 bedeutet:\n",
    "\n",
    "# Anzahl Sekunden seit dem 1. Januar 1970 um 00:00 UTC (Unix-Epoch)\n",
    "# Das ist ein weltweit genormter Zeitstandard â€“ unabhÃ¤ngig von Land oder Format.\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Set file + bank info\n",
    "FILE_PATH = \"data/ubs/ubsg_info.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"info\"\n",
    "\n",
    "# Output path\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILENAME = f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Format helpers\n",
    "def format_number(value):\n",
    "    if value is None or value != value or isinstance(value, (float, int)) and np.isnan(value):\n",
    "        return \"not available\"\n",
    "    if isinstance(value, (float, int)):\n",
    "        if abs(value) >= 1_000_000_000_000:\n",
    "            return f\"{value / 1_000_000_000_000:.2f} trillion USD\"\n",
    "        elif abs(value) >= 1_000_000_000:\n",
    "            return f\"{value / 1_000_000_000:.2f} billion USD\"\n",
    "        elif abs(value) >= 1_000_000:\n",
    "            return f\"{value / 1_000_000:.2f} million USD\"\n",
    "        else:\n",
    "            return f\"{value:.0f} USD\"\n",
    "    return str(value)\n",
    "\n",
    "def format_date(value):\n",
    "    try:\n",
    "        return datetime.utcfromtimestamp(int(value)).strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        return str(value)\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build fields from flat content\n",
    "flat_fields = []\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, (dict, list)):\n",
    "        continue\n",
    "\n",
    "    # Check for date fields\n",
    "    if \"date\" in key.lower() and isinstance(value, (int, float)):\n",
    "        value_str = format_date(value)\n",
    "    else:\n",
    "        value_str = format_number(value)\n",
    "\n",
    "    label = key.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "    flat_fields.append(f\"{label}: {value_str}\")\n",
    "\n",
    "# Add long summary at the beginning\n",
    "long_summary = data.get(\"longBusinessSummary\", None)\n",
    "if long_summary:\n",
    "    flat_fields.insert(0, long_summary)\n",
    "    flat_fields.insert(1, \"\\n--\\n\")\n",
    "\n",
    "# Add officer data\n",
    "officer_lines = []\n",
    "if \"companyOfficers\" in data and isinstance(data[\"companyOfficers\"], list):\n",
    "    for officer in data[\"companyOfficers\"]:\n",
    "        if \"name\" in officer:\n",
    "            name = officer[\"name\"]\n",
    "            title = officer.get(\"title\", \"unknown title\")\n",
    "            pay = format_number(officer.get(\"totalPay\", \"N/A\"))\n",
    "            officer_lines.append(f\"{name}, {title} â€“ Total Pay: {pay}\")\n",
    "\n",
    "if officer_lines:\n",
    "    flat_fields.append(\"\\nTop Executives:\")\n",
    "    flat_fields.extend(officer_lines)\n",
    "\n",
    "# Final chunk text\n",
    "text = \"\\n\".join(flat_fields)\n",
    "\n",
    "chunk = {\n",
    "    \"text\": text,\n",
    "    \"metadata\": {\n",
    "        \"bank\": BANK,\n",
    "        \"date\": \"n/a\",\n",
    "        \"source\": SOURCE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(chunk, out_file, ensure_ascii=False)\n",
    "    out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… Info chunk saved to:\\n{OUTPUT_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a4acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 150 insider transaction chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/jpm_insider_transactions_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r insider_transactions (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Datei-Pfade anpassen\n",
    "FILE_PATH = \"data/jpm/jpm_insider_transactions.json\"\n",
    "BANK = \"JPM\"\n",
    "SOURCE = \"insider_transactions\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# Lade JSON als DataFrame\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# NaNs ersetzen durch \"not available\"\n",
    "df = df.fillna(\"not available\")\n",
    "\n",
    "# Generiere Chunks\n",
    "chunks = []\n",
    "for _, row in df.iterrows():\n",
    "    date_str = row[\"Start Date\"]\n",
    "    text_lines = []\n",
    "    for col, val in row.items():\n",
    "        if col != \"index\" and col != \"Start Date\":\n",
    "            label = col.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "            text_lines.append(f\"{label}: {val}\")\n",
    "    chunk_text = \"\\n\".join(text_lines)\n",
    "\n",
    "    chunk = {\n",
    "        \"text\": chunk_text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date_str,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    }\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Speichern\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(chunks)} insider transaction chunks saved to:\\n{OUTPUT_FILE}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22044f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 10 institutional holder chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_institutional_holders_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r institutional_holders (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# File setup\n",
    "FILE_PATH = \"data/ubs/ubsg_institutional_holders.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"institutional_holders\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# Load as DataFrame\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "df = pd.DataFrame(records)\n",
    "df = df.fillna(\"not available\")\n",
    "\n",
    "# Build chunks\n",
    "chunks = []\n",
    "for _, row in df.iterrows():\n",
    "    date_str = row[\"Date Reported\"]\n",
    "    lines = []\n",
    "    for col, val in row.items():\n",
    "        if col in [\"index\", \"Date Reported\"]:\n",
    "            continue\n",
    "        label = col.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "        lines.append(f\"{label}: {val}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunk = {\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date_str,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    }\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Save as JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(chunks)} institutional holder chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52474fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 10 mutual fund holder chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_mutualfund_holders_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r mutualfund_holders (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# File paths & info\n",
    "FILE_PATH = \"data/ubs/ubsg_mutualfund_holders.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"mutualfund_holders\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# Load data\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "df = pd.DataFrame(records)\n",
    "df = df.fillna(\"not available\")\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for _, row in df.iterrows():\n",
    "    date_str = row[\"Date Reported\"]\n",
    "    lines = []\n",
    "    for col, val in row.items():\n",
    "        if col in [\"index\", \"Date Reported\"]:\n",
    "            continue\n",
    "        label = col.replace(\"_\", \" \").replace(\"  \", \" \").capitalize()\n",
    "        lines.append(f\"{label}: {val}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date_str,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save as .jsonl\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(chunks)} mutual fund holder chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b11e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 10 news chunks saved to:\n",
      "data_chunks/jpm_news_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r news (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Input & Output Setup\n",
    "FILE_PATH = \"data/jpm/jpm_news.json\"\n",
    "BANK = \"JPM\"\n",
    "SOURCE = \"news\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# Load JSON\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "# Generate Chunks\n",
    "chunks = []\n",
    "for item in records:\n",
    "    content = item.get(\"content\", {})\n",
    "    pub_date = content.get(\"pubDate\", \"unknown\")\n",
    "    title = content.get(\"title\", \"\").strip()\n",
    "    summary = content.get(\"summary\", \"\").strip()\n",
    "    thumbnail_url = content.get(\"thumbnail\", {}).get(\"originalUrl\", \"\")\n",
    "\n",
    "    # Textchunk zusammenbauen\n",
    "    text_parts = [\n",
    "        f\"Title: {title}\" if title else \"\",\n",
    "        f\"Summary: {summary}\" if summary else \"\",\n",
    "        f\"URL: {thumbnail_url}\" if thumbnail_url else \"\"\n",
    "    ]\n",
    "    chunk_text = \"\\n\".join([part for part in text_parts if part])\n",
    "\n",
    "    if chunk_text:  # Nur speichern wenn sinnvoller Inhalt vorhanden\n",
    "        chunk = {\n",
    "            \"text\": chunk_text,\n",
    "            \"metadata\": {\n",
    "                \"bank\": BANK,\n",
    "                \"date\": pub_date[:10],  # Nur YYYY-MM-DD\n",
    "                \"source\": SOURCE\n",
    "            }\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "\n",
    "# Save to .jsonl\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(chunks)} news chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79124da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 7576 share price chunks saved to:\n",
      "data_chunks/ubsg_shareprices_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r shareprices (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ðŸ”§ Pfade anpassen\n",
    "FILE_PATH = \"data/ubs/ubsg_shareprices.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"shareprices\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# JSON laden\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for row in data:\n",
    "    # Keys reparieren, falls Tuple-Strings wie \"('BAC', 'Open')\"\n",
    "    fixed_row = {}\n",
    "    for k, v in row.items():\n",
    "        if isinstance(k, str):\n",
    "            key_str = k\n",
    "        elif isinstance(k, (list, tuple)):\n",
    "            key_str = \"_\".join(str(part) for part in k)\n",
    "        else:\n",
    "            key_str = str(k)\n",
    "        fixed_row[key_str] = v\n",
    "\n",
    "    # Datum extrahieren â€“ versuche alle Varianten\n",
    "    possible_date_keys = [k for k in fixed_row.keys() if \"date\" in k.lower()]\n",
    "    date = \"unknown\"\n",
    "    for dk in possible_date_keys:\n",
    "        date_candidate = fixed_row.get(dk)\n",
    "        if isinstance(date_candidate, str) and len(date_candidate) >= 10:\n",
    "            date = date_candidate[:10]\n",
    "            break\n",
    "\n",
    "    # Text zusammenbauen\n",
    "    lines = [f\"Date: {date}\"]\n",
    "    for key, value in fixed_row.items():\n",
    "        if key.lower() not in [dk.lower() for dk in possible_date_keys]:\n",
    "            lines.append(f\"{key}: {value}\")\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    chunks.append({\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Speichern\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(chunks)} share price chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240a0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 53 Wikipedia chunks saved to:\n",
      "/Users/morenogallo/Library/Mobile Documents/com~apple~CloudDocs/Bachelorarbeit/Fact_Checking/data_chunks/ubsg_wikipedia_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# chunk generator fÃ¼r wikipedia (pfad und bank muss jeweils manuell angepasst werden)\n",
    "\n",
    "# warum man split_text() braucht:\n",
    "# In vielen RAG-Systemen ist es Ã¼blich, lange Texte in kleinere Einheiten (\"Chunks\") zu teilen, z.â€¯B.:\n",
    "\n",
    "# bessere Vektoreinbettung (Embeddings sind oft auf ~500â€“1000 Token beschrÃ¤nkt)\n",
    "# prÃ¤zisere Antworten auf Teilfragen\n",
    "# niedrigere KontextlÃ¤ngenbelastung bei LLM-Anfragen\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ðŸ”§ Wikipedia JSON-File & Infos\n",
    "FILE_PATH = \"data/ubs/ubsg_wikipedia.json\"\n",
    "BANK = \"UBSG\"\n",
    "SOURCE = \"wikipedia\"\n",
    "OUTPUT_FOLDER = \"data_chunks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, f\"{BANK.lower()}_{SOURCE}_chunks.jsonl\")\n",
    "\n",
    "# JSON laden\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Datum formatieren\n",
    "def parse_iso_date(iso_str):\n",
    "    try:\n",
    "        return datetime.fromisoformat(iso_str).strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "date = parse_iso_date(data.get(\"last_updated\", \"\"))\n",
    "title = data.get(\"title\", \"Unknown\")\n",
    "url = data.get(\"url\", \"Unknown\")\n",
    "extract = data.get(\"extract\", \"\").strip()\n",
    "\n",
    "# Wikipedia-Text chunken (z.â€¯B. alle 1000 WÃ¶rter als eigener Chunk)\n",
    "def split_text(text, max_words=200):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk_text = \" \".join(words[i:i + max_words])\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "text_chunks = split_text(extract)\n",
    "\n",
    "# RAG-kompatible JSONL-Chunks\n",
    "output_chunks = []\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    output_chunks.append({\n",
    "        \"text\": f\"{title} (Wikipedia excerpt {i+1})\\n\\n{chunk}\",\n",
    "        \"metadata\": {\n",
    "            \"bank\": BANK,\n",
    "            \"date\": date,\n",
    "            \"source\": SOURCE,\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"chunk_id\": i + 1\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Speichern\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for chunk in output_chunks:\n",
    "        json.dump(chunk, out_file, ensure_ascii=False)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… {len(output_chunks)} Wikipedia chunks saved to:\\n{OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
